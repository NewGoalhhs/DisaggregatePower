import os
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
import numpy as np
from core.MachineLearningModel import MachineLearningModel


class RandomForestModel(MachineLearningModel):
    def __init__(self, n_estimators=12, random_state=42):
        self.scaler = StandardScaler()
        self.encoder = OneHotEncoder(categories='auto', sparse_output=False)
        self.model = RandomForestClassifier(verbose=2, n_estimators=n_estimators, random_state=random_state, n_jobs=-1)

    def preprocess_data(self, data):
        df = pd.DataFrame(data)
        df['datetime'] = pd.to_datetime(df['datetime'])
        df = df.sort_values(by='datetime').reset_index(drop=True)

        # Check initial state of the dataframe
        if df.shape[0] < 26:
            raise ValueError("Not enough data to generate rolling features. Ensure at least 26 data points.")

        # Generate rolling features for the last 25 power usage values
        for i in range(1, 26):
            df[f'power_usage_lag_{i}'] = df['power_usage'][i]

        # Drop rows with NaN values generated by the rolling window
        df = df.dropna().reset_index(drop=True)
        if df.empty:
            raise ValueError("Dataframe is empty after dropping NaN values. Ensure there is enough data.")

        # Extract additional time features
        df['hour'] = df['datetime'].dt.hour
        df['day'] = df['datetime'].dt.dayofweek

        # One-hot encode day of the week
        day_of_week_encoded = self.encoder.fit_transform(df[['day']])

        # Combine all features
        feature_columns = [f'power_usage_lag_{i}' for i in range(1, 26)] + ['hour']
        X = df[feature_columns]
        X = np.hstack((X.values, day_of_week_encoded))
        X = self.scaler.fit_transform(X)
        y = df['appliance_in_use'].values
        return X, y

    def file_extension(self):
        return 'joblib'

    def save_model(self, path):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        joblib.dump(self.model, path)

    def load_model(self, path):
        self.model = joblib.load(path)

    def train(self, data, test_size=0.2, random_state=42, epochs=100):
        X, y = self.preprocess_data(data)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

        print("Training model...")
        self.model.fit(X_train, y_train)

        print("Evaluating model...")
        y_pred = self.model.predict(X_test)
        print(classification_report(y_test, y_pred))
        print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")

    def predict(self, data):
        X, _ = self.preprocess_data(data)
        predictions = self.model.predict(X)
        return predictions.tolist()

    def visualize(self):
        import matplotlib.pyplot as plt

        feature_importances = self.model.feature_importances_
        feature_names = [f'power_usage_lag_{i}' for i in range(1, 26)] + ['hour'] + [f'day_{i}' for i in range(
            self.encoder.categories_[0].size)]
        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
        importance_df = importance_df.sort_values(by='Importance', ascending=False)

        plt.figure(figsize=(12, 8))
        plt.barh(importance_df['Feature'], importance_df['Importance'])
        plt.xlabel('Importance')
        plt.ylabel('Feature')
        plt.title('Feature Importances')
        plt.gca().invert_yaxis()
        plt.show()


# Example usage
if __name__ == "__main__":
    data = {
        'datetime': [
                        '2023-05-01 12:34:56', '2023-05-01 13:45:07', '2023-05-02 14:56:18',
                        '2023-05-03 15:07:29', '2023-05-04 16:18:40', '2023-05-05 17:29:51',
                        '2023-05-06 18:40:02', '2023-05-07 19:51:13', '2023-05-08 20:02:24',
                        '2023-05-09 21:13:35', '2023-05-10 22:24:46', '2023-05-11 23:35:57',
                        '2023-05-12 00:46:08', '2023-05-13 01:57:19', '2023-05-14 02:08:30',
                        '2023-05-15 03:19:41', '2023-05-16 04:30:52', '2023-05-17 05:41:03',
                        '2023-05-18 06:52:14', '2023-05-19 07:03:25', '2023-05-20 08:14:36',
                        '2023-05-21 09:25:47', '2023-05-22 10:36:58', '2023-05-23 11:47:09',
                        '2023-05-24 12:58:20', '2023-05-25 14:09:31'
                    ] * 5,
        'power_usage': [
                           10, 20, 15, 30, 25, 18, 22, 19, 30, 24, 16, 28, 26, 15, 10, 20, 22,
                           25, 28, 30, 15, 12, 10, 22, 30, 25
                       ] * 5,
        'appliance_in_use': [
                                0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,
                                1, 0, 1, 0
                            ] * 5
    }

    model = RandomForestModel(n_estimators=100)
    model.train(data)
    predictions = model.predict(data)
    print(f'Predictions: {predictions}')
    print(f'Score: {accuracy_score(data["appliance_in_use"], predictions)}')
